{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSlhEBZNRP8a"
      },
      "source": [
        "# **Understanding Deep Generative Models with Generalized Empirical Likehoods**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGn8KmmlRNje"
      },
      "source": [
        "Copyright 2023 DeepMind Technologies Limited\n",
        "\n",
        "All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license. You may obtain a copy of the Apache 2.0 license at: https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY). You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the licenses for the specific language governing permissions and limitations under those licenses.\n",
        "\n",
        "This is not an official Google product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KF-NotYRX4v"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK4M6jWpQ4sO"
      },
      "outputs": [],
      "source": [
        "!pip install ml_collections\n",
        "\n",
        "import functools\n",
        "import io\n",
        "import os\n",
        "\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "from scipy.stats import entropy\n",
        "import timeit\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from enum import Enum\n",
        "from google.colab import auth as google_auth\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.set_verbosity(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vMQM7BxRgL7"
      },
      "source": [
        "# Copy Data from Google Cloud Bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLnnXm4TRjIb"
      },
      "source": [
        "Authenticate user, and list data in GCP bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HQyBR7yRhMv"
      },
      "outputs": [],
      "source": [
        "google_auth.authenticate_user()\n",
        "!gsutil ls gs://dm_gel_metric/cifar10_mode_drop_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuKJLj8RRuKv"
      },
      "source": [
        "Copy to local disk and list what is copied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpgSsgQ_RvsJ"
      },
      "outputs": [],
      "source": [
        "!mkdir -p cifar10_mode_drop_data\n",
        "!gsutil cp gs://dm_gel_metric/cifar10_mode_drop_data/*.npz cifar10_mode_drop_data/\n",
        "!ls cifar10_mode_drop_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqF3NzcWR46Y"
      },
      "source": [
        "# GEL Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gAlAWA3R7XR"
      },
      "source": [
        "## Helper Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPy-fA8KR5pc"
      },
      "outputs": [],
      "source": [
        "def approx_in_cvx_hull(\n",
        "    hull_points: np.array, test_point: np.array, eps: float=0.01):\n",
        "  \"\"\"Triangle alg. to see if test_point is in the convex hull of hull_points.\n",
        "\n",
        "  Implementation of Kalantari et al.,\n",
        "  \"Randomized triangle algorithms for convex hull membership\"\n",
        "\n",
        "  Args:\n",
        "    hull_points: (n_points, n_dim) matrix of hull points.\n",
        "    test_point: (n_dim,) vector which is the test point.\n",
        "    eps: epsilon tolerance\n",
        "  Returns:\n",
        "    in_hull: boolean (True) if test point is in the convex hull of hull_points.\n",
        "    pivot_point: pivot point of the algorithm.\n",
        "  \"\"\"\n",
        "  mean_hull_point = np.mean(hull_points, axis=0)\n",
        "  aug_hull_points = np.vstack([hull_points, mean_hull_point])\n",
        "\n",
        "  def calc_dists(hull_points, point):\n",
        "    return np.sqrt(np.sum((hull_points-point) ** 2, axis=1))\n",
        "\n",
        "  def nearest_point(test_point, pivot_point, hull_point):\n",
        "    alpha = (np.dot(test_point-pivot_point, hull_point-pivot_point) /\n",
        "             np.sum((hull_point-pivot_point) ** 2))\n",
        "    if alpha \u003e= 0.0 and alpha \u003c 1.0:\n",
        "      return (1.0 - alpha) * pivot_point + alpha * hull_point\n",
        "    return hull_point\n",
        "\n",
        "  def get_new_hull_point(sub_hull_points, pivot_point, test_point):\n",
        "    norm_vec = ((test_point - pivot_point) /\n",
        "                np.linalg.norm(test_point - pivot_point))\n",
        "    a_mat = sub_hull_points - pivot_point\n",
        "    a_mat /= np.linalg.norm(a_mat, axis=1)[:, np.newaxis]\n",
        "    return sub_hull_points[np.argmax(np.dot(a_mat, norm_vec)), :]\n",
        "\n",
        "  dist_vec = calc_dists(aug_hull_points, test_point)\n",
        "  radius = np.max(dist_vec)\n",
        "  pivot_point = aug_hull_points[np.argmin(dist_vec), :]\n",
        "  i = 0\n",
        "  while np.linalg.norm(pivot_point-test_point) \u003e eps * radius:\n",
        "    dist_vec_pivot = calc_dists(aug_hull_points, pivot_point)\n",
        "    if np.all(dist_vec_pivot \u003c dist_vec):\n",
        "      # convex hull condition failed\n",
        "      logging.info('Witness found at iter %d', i)\n",
        "      return False, pivot_point\n",
        "    sub_hull_points = aug_hull_points[dist_vec_pivot \u003e dist_vec, :]\n",
        "    hull_point = get_new_hull_point(sub_hull_points, pivot_point, test_point)\n",
        "    pivot_point = nearest_point(test_point, pivot_point, hull_point)\n",
        "    i += 1\n",
        "\n",
        "  logging.info('It is probably in the convex hull. Took %d iterations', i)\n",
        "  logging.info('Distance for iter %d is %.5f, radius %.5f', i,\n",
        "               np.linalg.norm(pivot_point-test_point) / radius, radius)\n",
        "  return True, pivot_point\n",
        "\n",
        "\n",
        "def is_in_cvx_hull(hull_points: np.array, test_point: np.array):\n",
        "  \"\"\"Checks to see if test_point is in the convex hull of hull_points.\n",
        "\n",
        "  Args:\n",
        "    hull_points: (n_points, n_dim) matrix of hull points.\n",
        "    test_point: (n_dim,) vector which is the test point.\n",
        "  Returns:\n",
        "    in_hull: boolean (True) if test point is in the convex hull of hull_points.\n",
        "  \"\"\"\n",
        "\n",
        "  n_points, n_dims = hull_points.shape\n",
        "  c = np.zeros((n_points,))\n",
        "  matrix_a_ub = -np.eye(n_points)\n",
        "  b_ub = np.zeros((n_points,))\n",
        "\n",
        "  matrix_a_eq = np.ones((n_dims+1, n_points))\n",
        "  matrix_a_eq[:n_dims, :] = hull_points.T\n",
        "\n",
        "  b_eq = np.ones((n_dims+1))\n",
        "  b_eq[:n_dims] = test_point\n",
        "\n",
        "  res = linprog(c, A_ub=matrix_a_ub, b_ub=b_ub, A_eq=matrix_a_eq, b_eq=b_eq)\n",
        "  logging.info(res.status)\n",
        "  if res.success:\n",
        "    log_prob = np.sum(np.log2(res.x))\n",
        "    logging.info('Initial log prob is %.5f', log_prob)\n",
        "  in_hull = res.success\n",
        "  return in_hull\n",
        "\n",
        "\n",
        "def convert_space_to_dims(features: np.array):\n",
        "  \"\"\"Flatten features to dimensions.\"\"\"\n",
        "  out = features.reshape([features.shape[0], -1])\n",
        "  return out\n",
        "\n",
        "\n",
        "def do_pca(features: np.array, pca_dim: int):\n",
        "  logging.info('Whitening...')\n",
        "  cov_mat = np.cov(features, rowvar=False)\n",
        "  v = np.linalg.eig(cov_mat)[1]\n",
        "  features = np.real(np.dot(features, v)[:, :pca_dim])\n",
        "  logging.info('Done')\n",
        "\n",
        "  return features\n",
        "\n",
        "def one_sample_emp_lik_iteration(feature_diffs: np.array, params: np.array):\n",
        "  \"\"\"Perform newton iteration for empirical likelihood.\n",
        "\n",
        "  Args:\n",
        "    feature_diffs: per-sample features for moment conditions.\n",
        "    params: current parameters for calculating empirical likelihood.\n",
        "  Returns:\n",
        "    params: parameters after a Newton step.\n",
        "    output_stats: dictionary of output statistics.\n",
        "  \"\"\"\n",
        "  num_examples = feature_diffs.shape[0]\n",
        "  z = 1.0 + np.dot(feature_diffs, params)\n",
        "  inv_n = 1.0 / num_examples\n",
        "\n",
        "  # positive part of the modified logarithm\n",
        "  w_pos = 1.0 / z[z \u003e= inv_n]\n",
        "  f_diff_pos = feature_diffs[z \u003e= inv_n, :] * w_pos[:, np.newaxis]\n",
        "\n",
        "  # negative part of the modified logarithm\n",
        "  w_neg = (2.0 - num_examples * z[z \u003c inv_n]) * num_examples\n",
        "  f_diff_neg = feature_diffs[z \u003c inv_n, :]\n",
        "  num_egs2 = num_examples ** 2\n",
        "\n",
        "  neg_hess = (np.dot(f_diff_pos.T, f_diff_pos)\n",
        "              + np.dot(f_diff_neg.T, f_diff_neg) * num_egs2)\n",
        "  sc_f_diff_neg = f_diff_neg * w_neg[:, np.newaxis]\n",
        "  log_grad = np.sum(f_diff_pos, axis=0) + np.sum(sc_f_diff_neg, axis=0)\n",
        "  log_grad_norm = np.linalg.norm(log_grad)\n",
        "\n",
        "  direction = np.linalg.solve(neg_hess, log_grad)\n",
        "  params += 1.0 * direction\n",
        "  n_out_of_domain = f_diff_neg.shape[0]\n",
        "\n",
        "  probs = 1.0 / (num_examples * (1.0 + np.dot(feature_diffs, params)))\n",
        "  log_lik = np.sum(np.log(probs))\n",
        "  output_stats = dict(\n",
        "      probs=probs, obj=log_lik,\n",
        "      n_out_of_domain=n_out_of_domain, log_grad_norm=log_grad_norm,)\n",
        "  return params, output_stats\n",
        "\n",
        "\n",
        "def hellinger_dist(p: np.array, q: np.array):\n",
        "  assert np.all(p \u003e= 0.0)\n",
        "  assert np.all(q \u003e= 0.0)\n",
        "  norm_p = p / np.sum(p)\n",
        "  norm_q = q / np.sum(q)\n",
        "  return np.sqrt(1. - np.sum(np.sqrt(norm_p * norm_q)))\n",
        "\n",
        "\n",
        "class GELStatus(Enum):\n",
        "  SOLVED = \"solved\"\n",
        "  NOT_IN_CONVEX_HULL = \"not_in_convex_hull\"\n",
        "  BOUNDARY = \"boundary\"\n",
        "  OPTIMIZATION_FAILURE = \"optimization_failure\"\n",
        "  RUNNING = \"running\"\n",
        "\n",
        "\n",
        "class GELObjective(Enum):\n",
        "  EMPIRICAL_LIKELIHOOD = 1\n",
        "  EXPONENTIAL_TILTING = 2\n",
        "  EUCLIDEAN_LIKELIHOOD = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la4ayxabSCZy"
      },
      "source": [
        "## One-Sample GEL code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lWkkRacSDGQ"
      },
      "outputs": [],
      "source": [
        "class OneSampleGEL(object):\n",
        "  def __init__(self, config, model_unprocessed_feats: np.array,\n",
        "               test_unprocessed_feats: np.array):\n",
        "    self.config = config\n",
        "    feature_diffs, test_features, model_features = self._preprocess_features(\n",
        "        model_unprocessed_feats, test_unprocessed_feats)\n",
        "    self.feature_diffs = feature_diffs\n",
        "    self.test_features = test_features\n",
        "    self.model_features = model_features\n",
        "\n",
        "    self._current_loss = np.inf\n",
        "    iter_func, output_stats, gel_status, params = self._init_optimizer()\n",
        "    self._iter_func = iter_func\n",
        "    self._output_stats = output_stats\n",
        "    self._status = gel_status  # termination conditions\n",
        "    self._params = params\n",
        "\n",
        "  def _init_optimizer(self):\n",
        "    num_examples, ndims = self.feature_diffs.shape\n",
        "    if self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD:\n",
        "      iter_func = self.emp_lik_iteration\n",
        "    elif self.config.obj_type == GELObjective.EXPONENTIAL_TILTING:\n",
        "      iter_func = self.exp_tilted_iteration\n",
        "    elif self.config.obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "      iter_func = self.euc_lik_iteration\n",
        "    else:\n",
        "      raise ValueError('Objective type %s not valid', self.config.obj_type)\n",
        "    probs = np.empty((num_examples,))\n",
        "    probs[:] = np.nan\n",
        "    output_stats = dict(probs=probs, obj=np.inf,\n",
        "                        n_out_of_domain=0, log_grad_norm=np.inf)\n",
        "    params = np.zeros((ndims,))\n",
        "    gel_status = GELStatus.RUNNING\n",
        "    # Check if the convex hull condition is satisfied.\n",
        "    # This condition does not apply to the Euc. Likelihood\n",
        "    if not self.config.obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "      if not approx_in_cvx_hull(self.feature_diffs, np.zeros((ndims,)))[0]:\n",
        "        gel_status = GELStatus.NOT_IN_CONVEX_HULL\n",
        "    return iter_func, output_stats, gel_status, params\n",
        "\n",
        "  def euc_lik_iteration(self):\n",
        "    \"\"\"Perform newton step for euclidean likelihood (which solves the problem).\n",
        "    \"\"\"\n",
        "    num_examples = self.feature_diffs.shape[0]\n",
        "    sample_cov = np.cov(self.test_features, rowvar=False, bias=True)\n",
        "    self._params = np.linalg.solve(\n",
        "        sample_cov, np.mean(self.feature_diffs, axis=0))\n",
        "    demeaned_test_features = self.test_features - np.mean(\n",
        "        self.test_features, axis=0)\n",
        "    probs = (1 - np.dot(demeaned_test_features, self._params)) / num_examples\n",
        "\n",
        "    # square is probably \"more\" correct but easier for per point\n",
        "    euc_dist = np.sum((probs - 1.0 / num_examples) ** 2) * num_examples\n",
        "    log_grad_norm, n_out_of_domain = 0.0, 0\n",
        "    self._output_stats = dict(\n",
        "        probs=probs, loss=euc_dist,\n",
        "        n_out_of_domain=n_out_of_domain, log_grad_norm=log_grad_norm,)\n",
        "\n",
        "  def exp_tilted_iteration(self):\n",
        "    \"\"\"Perform half newton step for exponential tilting objective.\"\"\"\n",
        "    num_examples = self.feature_diffs.shape[0]\n",
        "    w_exp_tilt = np.exp(np.dot(self.feature_diffs, self._params)) / num_examples\n",
        "    sc_f_diff = self.feature_diffs * w_exp_tilt[:, np.newaxis]\n",
        "\n",
        "    hess = np.dot(sc_f_diff.T, self.feature_diffs)\n",
        "    log_grad = np.sum(sc_f_diff, axis=0)\n",
        "    newton_step = np.linalg.solve(hess, log_grad)\n",
        "    log_grad_norm = np.linalg.norm(log_grad)\n",
        "\n",
        "    self._params -= 0.5 * newton_step\n",
        "    exp_weights = np.exp(np.dot(self.feature_diffs, self._params))\n",
        "    probs = exp_weights / np.sum(exp_weights)\n",
        "    n_out_of_domain = 0\n",
        "    ent = entropy(probs, base=2)\n",
        "    self._output_stats = dict(\n",
        "        probs=probs, loss=-ent,\n",
        "        n_out_of_domain=n_out_of_domain, log_grad_norm=log_grad_norm,)\n",
        "\n",
        "  def emp_lik_iteration(self):\n",
        "    \"\"\"Perform newton iteration for empirical likelihood objective.\n",
        "\n",
        "    Returns:\n",
        "      params: parameters after a Newton step.\n",
        "      output_stats: dictionary of output statistics.\n",
        "    \"\"\"\n",
        "    # The empirical likelihood iteration is in the helper function section\n",
        "    # since we use it for both the one-sample and two-sample versions\n",
        "    self._params, output_stats = one_sample_emp_lik_iteration(\n",
        "        self.feature_diffs, self._params)\n",
        "    del output_stats['obj']\n",
        "    output_stats['loss'] = -np.mean(np.log(self._output_stats['probs']))\n",
        "    self._output_stats = output_stats\n",
        "\n",
        "  def _print_stats(self, iter_i: int):\n",
        "    \"\"\"Prints statistics at the given iteration.\"\"\"\n",
        "    logging.info('Loss is at iter %d is %.8f',\n",
        "                 iter_i, self._output_stats['loss'])\n",
        "    logging.info('minimum probability is %.8f',\n",
        "                 np.min(self._output_stats['probs']))\n",
        "    logging.info('maximum probability is %.8f',\n",
        "                 np.max(self._output_stats['probs']))\n",
        "    logging.info('sum of probability is %.8f',\n",
        "                 np.sum(self._output_stats['probs']))\n",
        "    logging.info('no. not in domain is %d',\n",
        "                 self._output_stats['n_out_of_domain'])\n",
        "    logging.info('log grad norm is %.15f',\n",
        "                 self._output_stats['log_grad_norm'])\n",
        "\n",
        "  def _preprocess_features(self, pre_model_features: np.ndarray,\n",
        "                           pre_test_features: np.ndarray):\n",
        "    \"\"\"Loads and converts features for calculation gen. empirical likelihood.\n",
        "\n",
        "    Args:\n",
        "      pre_model_features: unpreprocessed model features.\n",
        "      pre_test_features: unpreprocessed test features.\n",
        "    Returns:\n",
        "      feature_diffs: per-sample moment conditions.\n",
        "      test_features: per-sample test features.\n",
        "      model_features: per-sample model features.\n",
        "    \"\"\"\n",
        "    if self.config.num_model_examples \u003e 0:\n",
        "      pre_model_features = pre_model_features[:self.config.num_model_examples]\n",
        "    assert len(pre_model_features.shape) == len(pre_test_features.shape)\n",
        "\n",
        "    # convert features of size [bs, h, w, c] -\u003e [bs, dim_new]\n",
        "    pre_model_features = convert_space_to_dims(pre_model_features)\n",
        "    pre_test_features = convert_space_to_dims(pre_test_features)\n",
        "\n",
        "    # Use fewer dimensions if flags ask us to.\n",
        "    cut_dim = min(self.config.cut_dim, np.prod(pre_test_features.shape[1:]))\n",
        "    ndims = pre_model_features.shape[1]\n",
        "    assert cut_dim \u003c= ndims, f'cut_dim {cut_dim} and ndims {ndims}'\n",
        "    model_features = pre_model_features[:, :cut_dim]\n",
        "    test_features = pre_test_features[:, :cut_dim]\n",
        "    assert test_features.shape[1] == model_features.shape[1]\n",
        "\n",
        "    model_means = np.mean(model_features, axis=0)\n",
        "    feature_diffs = test_features - model_means[np.newaxis, :]\n",
        "\n",
        "    if self.config.whiten:\n",
        "      assert self.config.obj_type != GELObjective.EUCLIDEAN_LIKELIHOOD\n",
        "      feature_diffs = do_pca(feature_diffs, self.config.pca_dim)\n",
        "\n",
        "    return feature_diffs, test_features, model_features\n",
        "\n",
        "  def _num_out_of_domain(self) -\u003e bool:\n",
        "    if np.any(self._output_stats['probs']) \u003c 0.0:\n",
        "      logging.info(\n",
        "          'Encountered negative probability, likely due to optimization. '\n",
        "          'This will likely be fixed in an iteration or two.')\n",
        "    if np.any(self._output_stats['probs']) \u003e 1.0:\n",
        "      logging.info(\n",
        "          'Encountered probability \u003e 1.0, likely due to optimization. '\n",
        "          'This will likely be fixed in an iteration or two.')\n",
        "    return self._output_stats['n_out_of_domain'] \u003e 0\n",
        "\n",
        "  def _check_norm_param_condition(self):\n",
        "    norm_params = np.linalg.norm(self._params)\n",
        "    if (self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD\n",
        "        and norm_params \u003e self.config.norm_param_tol):\n",
        "      logging.info('Mean likely near boundary since norm is high '\n",
        "                   '%.5f, so likelihood is -Inf (\u003c%.5f)',\n",
        "                   norm_params, np.sum(np.log(self._output_stats['probs'])))\n",
        "      self._status = GELStatus.BOUNDARY\n",
        "\n",
        "  def _check_if_solved(self, iteration: int):\n",
        "    if iteration % 10 == 1: self._print_stats(iteration)\n",
        "    if not np.isfinite(self._output_stats['loss']):\n",
        "      return\n",
        "    elif not np.isfinite(self._current_loss):\n",
        "      return\n",
        "    if (np.abs(self._output_stats['loss']-self._current_loss) \u003c self.config.tol\n",
        "        or self._output_stats['log_grad_norm'] \u003c self.config.grad_norm_tol):\n",
        "      logging.info('GEL calculation converged at iteration %d. Terminating.',\n",
        "                   iteration)\n",
        "      self._print_stats(iteration)\n",
        "      self._status = GELStatus.SOLVED\n",
        "\n",
        "  def _calculate_objective(self):\n",
        "    probs = self._output_stats['probs']\n",
        "    num_examples = probs.shape[0]\n",
        "    if self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD:\n",
        "      objective = np.mean(np.log(probs))\n",
        "      best_objective = -np.log(num_examples)\n",
        "      worst_objective = -np.inf\n",
        "    elif self.config.obj_type == GELObjective.EXPONENTIAL_TILTING:\n",
        "      objective = entropy(probs, base=2)\n",
        "      best_objective = np.log2(num_examples)\n",
        "      worst_objective = 0.0\n",
        "    elif self.config.obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "      objective = np.sum((probs - 1. / num_examples) ** 2) * num_examples\n",
        "      best_objective = 0.0\n",
        "      worst_objective = np.inf\n",
        "    else:\n",
        "      raise ValueError('Objective type %s not valid', self.config.obj_type)\n",
        "\n",
        "    return objective, best_objective, worst_objective\n",
        "\n",
        "  def calculate_divergence(self):\n",
        "    probs = self._output_stats['probs']\n",
        "    num_examples = probs.shape[0]\n",
        "    if self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD:\n",
        "      divergence = -np.mean(np.log(num_examples * probs))\n",
        "    elif self.config.obj_type == GELObjective.EXPONENTIAL_TILTING:\n",
        "      divergence = np.sum(probs * np.log(probs * num_examples))\n",
        "    elif self.config.obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "      divergence = np.sum((probs - 1. / num_examples) ** 2) * num_examples\n",
        "    else:\n",
        "      raise ValueError('Objective type %s not valid', self.config.obj_type)\n",
        "\n",
        "    return divergence\n",
        "\n",
        "  def _evaluate_gel_solution(self, elapsed_time: float = 0.0):\n",
        "    objective, best_objective, worst_objective = self._calculate_objective()\n",
        "    if self._status == GELStatus.SOLVED:\n",
        "      logging.info('solved... in %f seconds', elapsed_time)\n",
        "      logging.info('Final objective is %.5f', objective)\n",
        "      logging.info('Best objective is %.5f', best_objective)\n",
        "    elif self._status == GELStatus.BOUNDARY:\n",
        "      assert self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD\n",
        "      logging.info('final log lik is -Inf_boundary')\n",
        "    elif self._status == GELStatus.NOT_IN_CONVEX_HULL:\n",
        "      logging.info('Convex Hull condition not satisfied')\n",
        "      err = ('model mean not in convex hull of features... distributions not '\n",
        "             'close enough to GEL')\n",
        "      logging.info(err)\n",
        "    else:\n",
        "      logging.info('failed to solve... checking to see if model mean '\n",
        "                   'is in convex hull')\n",
        "      logging.info('calculating convex hull')\n",
        "      if not is_in_cvx_hull(test_features, model_means):\n",
        "        err = ('model mean not in convex hull of features... distributions not '\n",
        "               'close enough to GEL')\n",
        "        logging.info(err)\n",
        "        self._status = GELStatus.NOT_IN_CONVEX_HULL\n",
        "      else:\n",
        "        logging.info('optimization failed')\n",
        "        self._status = GELStatus.OPTIMIZATION_FAILED\n",
        "\n",
        "    if self._status != GELStatus.SOLVED:\n",
        "      objective = worst_objective\n",
        "      self._output_stats['probs'][:] = np.nan\n",
        "    out_dict = dict()\n",
        "    out_dict.update(self.config.to_dict())\n",
        "    out_dict['termination_status'] = self._status\n",
        "    out_dict['probs'] = self._output_stats['probs']\n",
        "    out_dict['objective'] = objective\n",
        "    out_dict['elapsed_time'] = elapsed_time\n",
        "    out_dict['model_feat_dims'] = self.model_features.shape\n",
        "    out_dict['test_feat_dims'] = self.test_features.shape\n",
        "\n",
        "    return out_dict\n",
        "\n",
        "  def calculate_gel(self):\n",
        "    \"\"\"Calculates empirical likelihood and output results to a dictionary.\"\"\"\n",
        "    if self._status != GELStatus.RUNNING:\n",
        "      return self._evaluate_gel_solution()\n",
        "\n",
        "    start_time = timeit.default_timer()\n",
        "    for i in range(self.config.num_iterations):\n",
        "      try:\n",
        "        self._iter_func()\n",
        "      except np.linalg.LinAlgError:  # This error indicates EL hit the boundary\n",
        "        logging.info('Encountered LinAlg Error, means we hit boundary cond')\n",
        "        self._status = GELStatus.BOUNDARY\n",
        "        break\n",
        "      self._check_norm_param_condition()\n",
        "      if self._status == GELStatus.BOUNDARY:\n",
        "        break\n",
        "      if self._num_out_of_domain(): continue\n",
        "      self._check_if_solved(i)\n",
        "      if self._status == GELStatus.SOLVED:\n",
        "        break\n",
        "      self._current_loss = self._output_stats['loss']\n",
        "\n",
        "    elapsed_time = timeit.default_timer() - start_time\n",
        "    return self._evaluate_gel_solution(elapsed_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfrU7abYSHDZ"
      },
      "source": [
        "## Two-Sample GEL Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6UKtBdMSHhN"
      },
      "outputs": [],
      "source": [
        "class TwoSampleGEL(object):\n",
        "  def __init__(self, config, model_unprocessed_feats: np.array,\n",
        "               test_unprocessed_feats: np.array):\n",
        "    self.config = config\n",
        "    res = self._preprocess_features(\n",
        "        model_unprocessed_feats, test_unprocessed_feats)\n",
        "    model_feats, test_feats, aug_model_feats, aug_test_feats, aug_feats = res\n",
        "    self.model_features = model_feats\n",
        "    self.test_features = test_feats\n",
        "    self.aug_features = aug_feats\n",
        "    self.aug_model_features = aug_model_feats\n",
        "    self.aug_test_features = aug_test_feats\n",
        "\n",
        "    self._current_loss = np.inf\n",
        "    iter_func, output_stats, gel_status, params = self._init_optimizer()\n",
        "    self._iter_func = iter_func\n",
        "    self._output_stats = output_stats\n",
        "    self._status = gel_status  # termination conditions\n",
        "    self._params = params\n",
        "\n",
        "  def _init_optimizer(self):\n",
        "    num_model_examples = self.model_features.shape[0]\n",
        "    num_test_examples = self.test_features.shape[0]\n",
        "    if self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD:\n",
        "      iter_func = self.emp_lik_iteration\n",
        "      ndims = self.aug_features.shape[1]\n",
        "    elif self.config.obj_type == GELObjective.EXPONENTIAL_TILTING:\n",
        "      iter_func = self.exp_tilted_iteration\n",
        "      ndims = self.aug_features.shape[1]\n",
        "    elif self.config.obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "      iter_func = self.euc_likelihood_iteration\n",
        "      ndims = self.model_features.shape[1]\n",
        "    else:\n",
        "      raise ValueError('Objective type %s not valid', self.config.obj_type)\n",
        "    model_probs = np.empty((num_model_examples,))\n",
        "    model_probs[:] = np.nan\n",
        "    test_probs = np.empty((num_test_examples,))\n",
        "    test_probs[:] = np.nan\n",
        "    output_stats = dict(\n",
        "        model_probs=model_probs, test_probs=test_probs,\n",
        "        model_loss=np.inf, test_loss=np.inf,\n",
        "        n_out_of_domain=0, log_grad_norm=np.nan)\n",
        "\n",
        "    params = np.zeros((ndims,))\n",
        "\n",
        "    gel_status = GELStatus.RUNNING\n",
        "    # Check if the convex hull condition is satisfied.\n",
        "    # This condition does not apply to the Euc. Likelihood\n",
        "    if not self.config.obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "      if not approx_in_cvx_hull(\n",
        "          self.aug_features, np.zeros((self.aug_features.shape[1],)))[0]:\n",
        "        gel_status = GELStatus.NOT_IN_CONVEX_HULL\n",
        "    return iter_func, output_stats, gel_status, params\n",
        "\n",
        "  def euc_likelihood_iteration(self):\n",
        "    \"\"\"Perform one (and only) newton iteration for two-sample\n",
        "       euclidean likelihood.\n",
        "    \"\"\"\n",
        "    model_egs = self.model_features.shape[0]\n",
        "    test_egs = self.test_features.shape[0]\n",
        "    if model_egs != test_egs:\n",
        "      raise ValueError(\n",
        "          \"Different number of model and test examples currently not supported\")\n",
        "    model_mean = np.mean(self.model_features, axis=0)\n",
        "    test_mean = np.mean(self.test_features, axis=0)\n",
        "    model_cov = np.cov(self.model_features, rowvar=False, bias=True)\n",
        "    test_cov = np.cov(self.test_features, rowvar=False, bias=True)\n",
        "    sample_cov = (model_egs * model_cov + test_egs * test_cov) / (\n",
        "        model_egs + test_egs)\n",
        "    self._params = np.linalg.solve(sample_cov, model_mean - test_mean)\n",
        "    model_probs = 1. / model_egs - np.dot(\n",
        "        self.model_features - model_mean, self._params)\n",
        "    test_probs = 1. / test_egs + np.dot(\n",
        "        self.test_features - test_mean, self._params)\n",
        "\n",
        "    model_obj = 0.5 * np.mean((model_probs - 1.0 / model_egs) ** 2)\n",
        "    test_obj = 0.5 * np.mean((test_probs - 1.0 / test_egs) ** 2)\n",
        "\n",
        "    self._output_stats = dict(\n",
        "        model_probs=model_probs,\n",
        "        test_probs=test_probs,\n",
        "        model_loss=model_obj,\n",
        "        test_loss=test_obj,\n",
        "        n_out_of_domain=0,\n",
        "        log_grad_norm=0.0,  # optimized with one step\n",
        "        )\n",
        "\n",
        "  def emp_lik_iteration(self):\n",
        "    \"\"\"Perform newton step for two-sample empirical likelihood.\n",
        "\n",
        "    Standard two-sample GEL methods are in the form:\n",
        "    sum_i log(p_i) + sum_j log(q_j)\n",
        "    s.t.\n",
        "    sum_i p_i = 1\n",
        "    sum_j q_j = 1\n",
        "    sum_i p_i X_i = sum_j q_j Y_j\n",
        "\n",
        "    This is can recast in the form:\n",
        "    sum_k log(w_k)\n",
        "    s.t.\n",
        "    sum_k w_k = 1\n",
        "    sum_k w_k Z_k = 0\n",
        "\n",
        "    where w_k = 0.5 * p_k for k=1,...,N\n",
        "    and w_{k+N} = 0.5 * q_k for k=1,...,M\n",
        "\n",
        "    Z_k = [X_k]  for k=1,...,N\n",
        "          [1  ]\n",
        "    and\n",
        "    Z_{k+N} = [-Y_k]  for k=1,...,M\n",
        "              [-1  ]\n",
        "    \"\"\"\n",
        "    self._params, output_stats = one_sample_emp_lik_iteration(\n",
        "        self.aug_features, self._params)\n",
        "    model_egs = model_features.shape[0]\n",
        "    test_egs = test_features.shape[0]\n",
        "    model_probs = 2.0 * output_stats['probs'][:model_egs]\n",
        "    test_probs = 2.0 * output_stats['probs'][model_egs:]\n",
        "    model_log_lik = np.mean(np.log(model_probs))\n",
        "    test_log_lik = np.mean(np.log(test_probs))\n",
        "\n",
        "    self._output_stats = dict(\n",
        "        model_probs=model_probs,\n",
        "        test_probs=test_probs,\n",
        "        model_loss=-model_log_lik,\n",
        "        test_loss=-test_log_lik,\n",
        "        n_out_of_domain=output_stats['n_out_of_domain'],\n",
        "        log_grad_norm=output_stats['log_grad_norm'],\n",
        "        )\n",
        "\n",
        "  def exp_tilted_iteration(self):\n",
        "    \"\"\"Performs half Newton step on two-sample exp. tilting objective.\n",
        "\n",
        "    See Appendix C.2 for details of the implementation.\n",
        "    \"\"\"\n",
        "    model_egs = self.model_features.shape[0]\n",
        "    test_egs = self.test_features.shape[0]\n",
        "    if model_egs != test_egs:\n",
        "      raise ValueError(\n",
        "          \"Different number of model and test examples currently not supported\")\n",
        "    num_examples = model_egs + test_egs\n",
        "\n",
        "    w_exp_tilt = np.exp(np.dot(self.aug_features, self._params)) / num_examples\n",
        "    sc_f_diff = self.aug_features * w_exp_tilt[:, np.newaxis]\n",
        "\n",
        "    hess = np.dot(sc_f_diff.T, self.aug_features)\n",
        "    log_grad = np.sum(sc_f_diff, axis=0)\n",
        "    log_grad_norm = np.linalg.norm(log_grad)\n",
        "    newton_step = np.linalg.solve(hess, log_grad)\n",
        "    self._params -= 0.5 * newton_step\n",
        "    exp_weights = np.concatenate(\n",
        "        [np.exp(np.dot(self.aug_model_features, self._params)) * test_egs,\n",
        "        np.exp(np.dot(-self.aug_test_features, self._params)) * model_egs])\n",
        "    probs = exp_weights / np.sum(exp_weights)\n",
        "    model_probs = 2 * probs[:model_egs]\n",
        "    test_probs = 2 * probs[model_egs:]\n",
        "    self._output_stats = dict(\n",
        "        model_probs=model_probs,\n",
        "        test_probs=test_probs,\n",
        "        model_loss=-entropy(model_probs, base=2),\n",
        "        test_loss=-entropy(test_probs, base=2),\n",
        "        n_out_of_domain=0,\n",
        "        log_grad_norm=log_grad_norm,\n",
        "        )\n",
        "\n",
        "  def _preprocess_features(self, model_features, test_features):\n",
        "    \"\"\"Loads and converts features for calculate_two_sample_gel.\"\"\"\n",
        "    model_egs = model_features.shape[0]\n",
        "    test_egs = test_features.shape[0]\n",
        "\n",
        "    two_sample_features = np.concatenate(\n",
        "        [model_features, -test_features], axis=0)\n",
        "    if self.config.whiten:\n",
        "      assert self.config.obj_type != GELObjective.EUCLIDEAN_LIKELIHOOD\n",
        "      two_sample_features = do_pca(two_sample_features, self.config.pca_dim)\n",
        "\n",
        "    out_model_feats = two_sample_features[:model_egs]\n",
        "    out_test_feats = -two_sample_features[-test_egs:]\n",
        "    aug_model_feats, aug_test_feats, aug_feats = self._make_augmented_features(\n",
        "        out_model_feats, out_test_feats)\n",
        "\n",
        "    return (out_model_feats, out_test_feats, aug_model_feats, aug_test_feats,\n",
        "            aug_feats)\n",
        "\n",
        "  def _make_augmented_features(self, model_features, test_features):\n",
        "    \"\"\"Make two-sample features for use with empirical likelihood iteration.\"\"\"\n",
        "    model_egs = model_features.shape[0]\n",
        "    test_egs = test_features.shape[0]\n",
        "\n",
        "    aug_model_feats = np.concatenate(\n",
        "        [model_features, np.ones((model_egs, 1))], axis=1)\n",
        "    aug_test_feats = np.concatenate(\n",
        "        [test_features, np.ones((test_egs, 1))], axis=1)\n",
        "\n",
        "    aug_features = np.concatenate([aug_model_feats, -aug_test_feats], axis=0)\n",
        "\n",
        "    return aug_model_feats, aug_test_feats, aug_features\n",
        "\n",
        "  def _print_stats(self, iter_i):\n",
        "    \"\"\"Print statistics during optimization.\"\"\"\n",
        "    logging.info('model loss is at iter %d is %.8f',\n",
        "                 iter_i, self._output_stats['model_loss'])\n",
        "    logging.info('test loss is at iter %d is %.8f',\n",
        "                 iter_i, self._output_stats['test_loss'])\n",
        "    logging.info('minimum model probability is %.8f',\n",
        "                 np.min(self._output_stats['model_probs']))\n",
        "    logging.info('maximum model probability is %.8f',\n",
        "                 np.max(self._output_stats['model_probs']))\n",
        "    logging.info('sum of model probabilities is %.8f',\n",
        "                 np.sum(self._output_stats['model_probs']))\n",
        "    logging.info('minimum test probability is %.8f',\n",
        "                 np.min(self._output_stats['test_probs']))\n",
        "    logging.info('maximum test probability is %.8f',\n",
        "                 np.max(self._output_stats['test_probs']))\n",
        "    logging.info('sum of test probabilities is %.8f',\n",
        "                 np.sum(self._output_stats['test_probs']))\n",
        "    logging.info(\n",
        "        'number not in domain is %d', self._output_stats['n_out_of_domain'])\n",
        "    logging.info(\n",
        "        'log grad norm is %.15f', self._output_stats['log_grad_norm'])\n",
        "\n",
        "\n",
        "  def _num_out_of_domain(self) -\u003e bool:\n",
        "    if np.any(self._output_stats['model_probs']) \u003c 0.0:\n",
        "      logging.info(\n",
        "          'Encountered negative model probability, likely due to '\n",
        "          'optimization. This will likely be fixed in an iteration or two.')\n",
        "    if np.any(self._output_stats['model_probs']) \u003e 1.0:\n",
        "      logging.info(\n",
        "          'Encountered model probability \u003e 1.0, likely due to optimization.'\n",
        "          ' This will likely be fixed in an iteration or two.')\n",
        "    if np.any(self._output_stats['test_probs']) \u003c 0.0:\n",
        "      logging.info(\n",
        "          'Encountered negative test probability, likely due to '\n",
        "          'optimization. This will likely be fixed in an iteration or two.')\n",
        "    if np.any(self._output_stats['test_probs']) \u003e 1.0:\n",
        "      logging.info(\n",
        "          'Encountered test probability \u003e 1.0, likely due to optimization. '\n",
        "          'This will likely be fixed in an iteration or two.')\n",
        "\n",
        "    return self._output_stats['n_out_of_domain'] \u003e 0\n",
        "\n",
        "  def _check_norm_param_condition(self):\n",
        "    norm_params = np.linalg.norm(self._params)\n",
        "    if (self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD\n",
        "        and norm_params \u003e self.config.norm_param_tol):\n",
        "      model_ll = np.sum(np.log(self._output_stats['model_probs']))\n",
        "      test_ll = np.sum(np.log(self._output_stats['test_probs']))\n",
        "      logging.info(\n",
        "          'Mean likely near boundary since norm is high %.5f, '\n",
        "          'so model likelihood is -Inf, \u003c%.5f, as is test likelihood, \u003c%.5f.',\n",
        "          norm_params, model_ll, test_ll)\n",
        "      self._status = GELStatus.BOUNDARY\n",
        "\n",
        "  def _check_if_solved(self, iteration: int):\n",
        "    if iteration % 10 == 1: self._print_stats(iteration)\n",
        "    obj = self._output_stats['model_loss'] + self._output_stats['test_loss']\n",
        "    if not(np.isfinite(obj) and np.isfinite(self._current_loss)): return\n",
        "    if (np.abs(obj-self._current_loss) \u003c self.config.tol\n",
        "        or self._output_stats['log_grad_norm'] \u003c self.config.grad_norm_tol):\n",
        "      logging.info('GEL calculation converged '\n",
        "                   'at iteration %d... terminating', iteration)\n",
        "      self._print_stats(iteration)\n",
        "      self._status = GELStatus.SOLVED\n",
        "\n",
        "  def _calculate_objective(self):\n",
        "    model_probs = self._output_stats['model_probs']\n",
        "    test_probs = self._output_stats['test_probs']\n",
        "    num_model_examples = model_probs.shape[0]\n",
        "    num_test_examples = test_probs.shape[0]\n",
        "    if self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD:\n",
        "      model_objective = np.mean(np.log(model_probs))\n",
        "      test_objective = np.mean(np.log(test_probs))\n",
        "      best_objective = -np.log(num_model_examples) - np.log(num_test_examples)\n",
        "      worst_objective = -np.inf\n",
        "    elif self.config.obj_type == GELObjective.EXPONENTIAL_TILTING:\n",
        "      model_objective = entropy(model_probs, base=2)\n",
        "      test_objective = entropy(test_probs, base=2)\n",
        "      best_objective = np.log2(num_test_examples) + np.log2(num_model_examples)\n",
        "      worst_objective = 0.0\n",
        "    elif self.config.obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "      model_objective = self._output_stats['model_loss']\n",
        "      test_objective = self._output_stats['test_loss']\n",
        "      best_objective = 0.0\n",
        "      worst_objective = np.inf\n",
        "    else:\n",
        "      raise ValueError('Objective type %s not valid', self.config.obj_type)\n",
        "\n",
        "    objective = model_objective + test_objective\n",
        "    return objective, best_objective, worst_objective\n",
        "\n",
        "  def calculate_divergence(self):\n",
        "    model_probs = self._output_stats['model_probs']\n",
        "    test_probs = self._output_stats['test_probs']\n",
        "    num_model_examples = model_probs.shape[0]\n",
        "    num_test_examples = test_probs.shape[0]\n",
        "    if self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD:\n",
        "      model_divergence = -np.mean(np.log(num_model_examples * model_probs))\n",
        "      test_divergence = -np.mean(np.log(num_test_examples * test_probs))\n",
        "      divergence = model_divergence + test_divergence\n",
        "    elif self.config.obj_type == GELObjective.EXPONENTIAL_TILTING:\n",
        "      model_divergence = np.sum(\n",
        "          model_probs * np.log(model_probs * num_model_examples))\n",
        "      test_divergence = np.sum(\n",
        "          test_probs * np.log(test_probs * num_test_examples))\n",
        "    elif self.config.obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "      model_divergence = 0.5 * np.mean(\n",
        "          (1. / num_model_examples - model_probs) ** 2)\n",
        "      test_divergence = 0.5 * np.mean(\n",
        "          (1. / num_test_examples - test_probs) ** 2)\n",
        "    else:\n",
        "      raise ValueError('Objective type %s not valid', self.config.obj_type)\n",
        "\n",
        "    divergence = model_divergence + test_divergence\n",
        "    return divergence, model_divergence, test_divergence\n",
        "\n",
        "  def _evaluate_gel_solution(self, elapsed_time: float = 0.0):\n",
        "    if self._status == GELStatus.SOLVED:\n",
        "      logging.info('solved... in %f seconds', elapsed_time)\n",
        "    elif self._status == GELStatus.BOUNDARY:\n",
        "      assert self.config.obj_type == GELObjective.EMPIRICAL_LIKELIHOOD\n",
        "      logging.info('final log lik is -Inf_boundary')\n",
        "    elif self._status == GELStatus.NOT_IN_CONVEX_HULL:\n",
        "      logging.info('Convex Hull condition broken')\n",
        "      err = ('model mean not in convex hull of features... distributions not '\n",
        "             'close enough to GEL')\n",
        "      logging.info(err)\n",
        "    else:\n",
        "      logging.info('failed to solve... checking to see if model mean '\n",
        "                   'is in convex hull')\n",
        "      logging.info('calculating convex hull')\n",
        "      if not is_in_cvx_hull(self.aug_features,\n",
        "                            np.zeros((self.aug_features.shape[1],))):\n",
        "        err = ('model mean not in convex hull of features... distributions not '\n",
        "               'close enough to GEL')\n",
        "        logging.info(err)\n",
        "        self._status = GELStatus.NOT_IN_CONVEX_HULL\n",
        "      else:\n",
        "        logging.info('optimization failed')\n",
        "        self._status = GELStatus.OPTIMIZATION_FAILED\n",
        "\n",
        "    if self._status != GELStatus.SOLVED:\n",
        "      _, _, worst_objective = self._calculate_objective()\n",
        "      objective = worst_objective\n",
        "      self._output_stats['model_probs'][:] = np.nan\n",
        "      self._output_stats['test_probs'][:] = np.nan\n",
        "\n",
        "    out_dict = dict()\n",
        "    out_dict.update(self.config.to_dict())\n",
        "    out_dict['termination_status'] = self._status\n",
        "    out_dict['model_probs'] = self._output_stats['model_probs']\n",
        "    out_dict['test_probs'] = self._output_stats['test_probs']\n",
        "    objective, _, _ = self._calculate_objective()\n",
        "    out_dict['objective'] = objective\n",
        "    out_dict['elapsed_time'] = elapsed_time\n",
        "    out_dict['model_feat_dims'] = self.model_features.shape\n",
        "    out_dict['test_feat_dims'] = self.test_features.shape\n",
        "\n",
        "    return out_dict\n",
        "\n",
        "  def calculate_gel(self):\n",
        "    \"\"\"Calculates empirical likelihood and output results to a dictionary.\"\"\"\n",
        "    if self._status != GELStatus.RUNNING:\n",
        "      return self._evaluate_gel_solution()\n",
        "\n",
        "    start_time = timeit.default_timer()\n",
        "    for i in range(self.config.num_iterations):\n",
        "      try:\n",
        "        self._iter_func()\n",
        "      except np.linalg.LinAlgError:  # This error indicates EL hit the boundary\n",
        "        logging.info('Encountered LinAlg Error, means we hit boundary cond')\n",
        "        self._status = GELStatus.BOUNDARY\n",
        "        break\n",
        "      self._check_norm_param_condition()\n",
        "      if self._status == GELStatus.BOUNDARY:\n",
        "        break\n",
        "      if self._num_out_of_domain(): continue\n",
        "      self._check_if_solved(i)\n",
        "      if self._status == GELStatus.SOLVED:\n",
        "        break\n",
        "      self._current_loss = (self._output_stats['model_loss'] +\n",
        "                            self._output_stats['test_loss'])\n",
        "\n",
        "    elapsed_time = timeit.default_timer() - start_time\n",
        "    return self._evaluate_gel_solution(elapsed_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibeE17-4SN_P"
      },
      "source": [
        "## Kernel Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OFpbJiDSQM7"
      },
      "outputs": [],
      "source": [
        "def get_kernel_config():\n",
        "  \"\"\"Flags for the kernel config.\"\"\"\n",
        "  config = config_dict.ConfigDict()\n",
        "  config.kernel_type = 'exponential'\n",
        "  config.polynomial_params = config_dict.create(order=3, const=1.0)\n",
        "  config.exponential_params = config_dict.create(sigma=1.0)\n",
        "  config.laplacian_params = config_dict.create(sigma=1.0)\n",
        "  config.rbf_params = config_dict.create(sigma=1.0)\n",
        "  config.rational_quadratic_params = config_dict.create(order=2.0, const=1.0)\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def kernel_matrix(feat1: np.ndarray, feat2: np.ndarray,\n",
        "                  config: config_dict.ConfigDict):\n",
        "  \"\"\"Generate kernel matrix from two sets of features.\"\"\"\n",
        "  kernel_type = config.kernel_type.lower()\n",
        "  ndim = feat1.shape[1]\n",
        "\n",
        "  if kernel_type == 'linear':\n",
        "    kernel_mat = np.dot(feat1, feat2.T) / ndim\n",
        "  elif kernel_type == 'exponential':\n",
        "    sigma = config.exponential_params.sigma\n",
        "    assert sigma \u003e 0.0\n",
        "    kernel_mat = np.exp(sigma * np.dot(feat1, feat2.T) / ndim)\n",
        "  elif kernel_type == 'polynomial':\n",
        "    order = config.polynomial_params.order\n",
        "    const = config.polynomial_params.const\n",
        "    assert order \u003e 0.0 and const \u003e 0.0\n",
        "    ndim = feat1.shape[1]\n",
        "    kernel_mat = (np.dot(feat1, feat2.T) / ndim + const) ** order\n",
        "  elif kernel_type == 'laplacian' or 'laplace':\n",
        "    sigma = config.laplacian_params.sigma\n",
        "    assert sigma \u003e 0.0\n",
        "    dist_mat = pairwise_distances(feat1, feat2, metric='l1')\n",
        "    kernel_mat = np.exp(-sigma * dist_mat)\n",
        "  elif kernel_type == 'rbf' or 'gaussian':\n",
        "    sigma = config.rbf_params.sigma\n",
        "    assert sigma \u003e 0.0\n",
        "    dist_mat = pairwise_distances(feat1, feat2, metric='l2')\n",
        "    kernel_mat = np.exp(-sigma * (dist_mat ** 2))\n",
        "  elif kernel_type == 'rational_quadratic':\n",
        "    const = config.rational_quadratic_params.const\n",
        "    order = config.rational_quadratic_params.order\n",
        "    assert order \u003e 0.0 and const \u003e 0.0\n",
        "    squared_dist = pairwise_distances(feat1, feat2, metric='l2') ** 2\n",
        "    kernel_mat = (squared_dist * (const ** 2)) ** -order\n",
        "  else:\n",
        "    raise ValueError(f'kernel_type {kernel_type} not supported')\n",
        "\n",
        "  return kernel_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwhZP9QnSYmc"
      },
      "source": [
        "## Config Flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLTlJPMQSa0A"
      },
      "outputs": [],
      "source": [
        "def get_gel_config():\n",
        "  \"\"\"Flags for GEL calculation.\n",
        "\n",
        "  Returns:\n",
        "    config: ConfigDict for Flags\n",
        "  \"\"\"\n",
        "  config = config_dict.ConfigDict()\n",
        "  config.cut_dim = 1024\n",
        "  config.pca_dim = 1024\n",
        "  config.whiten = True\n",
        "  config.num_model_examples = 0\n",
        "  config.obj_type = GELObjective.EXPONENTIAL_TILTING\n",
        "\n",
        "  config.num_iterations = 10000\n",
        "  config.norm_param_tol = 1E8\n",
        "  config.tol = 1E-8\n",
        "  config.grad_norm_tol = 1E-8\n",
        "\n",
        "  return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALHI9NmxSixs"
      },
      "source": [
        "# Unit Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "583aBhFWSkjk"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC5LT9dNSg5T"
      },
      "outputs": [],
      "source": [
        "def calculate_gel_unit_tests(\n",
        "    shifts, obj_type, is_one_sample_test: bool = True):\n",
        "  config_flags = get_gel_config()\n",
        "  config_flags.obj_type = obj_type\n",
        "  if obj_type == GELObjective.EUCLIDEAN_LIKELIHOOD:\n",
        "    config_flags.whiten = False\n",
        "  else:\n",
        "    config_flags.whiten = True\n",
        "  if is_one_sample_test:\n",
        "    test_class = OneSampleGEL\n",
        "  else:\n",
        "    test_class = TwoSampleGEL\n",
        "  gel_one_sample_tests = list()\n",
        "  for shift in shifts:\n",
        "    gel_one_sample_test = test_class(\n",
        "        config_flags, model_features + shift, test_features)\n",
        "    gel_one_sample_test.calculate_gel()\n",
        "    gel_one_sample_tests.append(gel_one_sample_test)\n",
        "\n",
        "  return gel_one_sample_tests, config_flags\n",
        "\n",
        "\n",
        "def print_one_sample_unit_test_stats(\n",
        "    shifts, gel_one_sample_tests, config: config_dict.ConfigDict):\n",
        "  logging.info('Divergences for %s objective:', config.obj_type)\n",
        "  for shift, gel_one_sample_test in zip(shifts, gel_one_sample_tests):\n",
        "    logging.info('Shift is %f', shift)\n",
        "    solution_status = gel_one_sample_test._status\n",
        "    logging.info('Solution Status: %s', solution_status)\n",
        "    if solution_status == GELStatus.SOLVED:\n",
        "      logging.info(\n",
        "          'Divergence is %f', gel_one_sample_test.calculate_divergence())\n",
        "    logging.info('---------------------------------------------')\n",
        "\n",
        "\n",
        "def print_two_sample_unit_test_stats(\n",
        "    shifts, gel_two_sample_tests, config: config_dict.ConfigDict):\n",
        "  logging.info('Divergences for %s objective:', config.obj_type)\n",
        "  for shift, gel_two_sample_test in zip(shifts, gel_two_sample_tests):\n",
        "    logging.info('Shift is %f', shift)\n",
        "    solution_status = gel_two_sample_test._status\n",
        "    logging.info('Solution Status: %s', solution_status)\n",
        "    res = gel_two_sample_test.calculate_divergence()\n",
        "    divergence, model_divergence, test_divergence = res\n",
        "    if solution_status == GELStatus.SOLVED:\n",
        "      logging.info('Divergence is %f', divergence)\n",
        "      logging.info('Model divergence is %f', model_divergence)\n",
        "      logging.info('Test divergence is %f', test_divergence)\n",
        "    logging.info('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ZSQPXySsjv"
      },
      "source": [
        "## Load features and mean shift hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq6c1Rm0StvX"
      },
      "outputs": [],
      "source": [
        "model_features = np.random.randn(*(50000, 128))\n",
        "test_features = np.random.randn(*(50000, 128))\n",
        "\n",
        "shifts = [0.0, 0.1, 0.3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyMcdEjISvWH"
      },
      "source": [
        "## One-Sample GEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofgxe2FnSxoG"
      },
      "source": [
        "Empirical Likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK6gPXQsS0K1"
      },
      "outputs": [],
      "source": [
        "gel_one_sample_tests, config_flags = calculate_gel_unit_tests(\n",
        "    shifts, GELObjective.EMPIRICAL_LIKELIHOOD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZqt0qPqS2ci"
      },
      "outputs": [],
      "source": [
        "print_one_sample_unit_test_stats(shifts, gel_one_sample_tests, config_flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfelAGaKS3uk"
      },
      "source": [
        "Exponential Tilting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0jWxdMnS6MI"
      },
      "outputs": [],
      "source": [
        "gel_one_sample_tests, config_flags = calculate_gel_unit_tests(\n",
        "    shifts, GELObjective.EXPONENTIAL_TILTING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41wn41WRS9Mg"
      },
      "outputs": [],
      "source": [
        "print_one_sample_unit_test_stats(shifts, gel_one_sample_tests, config_flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGABwRt1S-HK"
      },
      "source": [
        "Euclidean Likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQkkYQGUTAMs"
      },
      "outputs": [],
      "source": [
        "gel_one_sample_tests, config_flags = calculate_gel_unit_tests(\n",
        "    shifts, GELObjective.EUCLIDEAN_LIKELIHOOD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5vfd9teTC1Y"
      },
      "outputs": [],
      "source": [
        "print_one_sample_unit_test_stats(shifts, gel_one_sample_tests, config_flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQzMkwXDTMYF"
      },
      "source": [
        "## Two-Sample GEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9jZa8v4TQTF"
      },
      "source": [
        "Empirical Likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxIhZQ8eTHtT"
      },
      "outputs": [],
      "source": [
        "gel_two_sample_tests, config_flags = calculate_gel_unit_tests(\n",
        "    shifts, GELObjective.EMPIRICAL_LIKELIHOOD, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tei2QXeSTTYl"
      },
      "outputs": [],
      "source": [
        "print_two_sample_unit_test_stats(shifts, gel_two_sample_tests, config_flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfyUYq6xTVpg"
      },
      "source": [
        "Exponential Tilting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RYkOIJ7TXmC"
      },
      "outputs": [],
      "source": [
        "gel_two_sample_tests, config_flags = calculate_gel_unit_tests(\n",
        "    shifts, GELObjective.EXPONENTIAL_TILTING, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5l_QNQpTmHC"
      },
      "outputs": [],
      "source": [
        "print_two_sample_unit_test_stats(shifts, gel_two_sample_tests, config_flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ01W0w6TpHr"
      },
      "source": [
        "Euclidean Likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBk7t5UGTn3R"
      },
      "outputs": [],
      "source": [
        "gel_two_sample_tests, config_flags = calculate_gel_unit_tests(\n",
        "    shifts, GELObjective.EUCLIDEAN_LIKELIHOOD, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XVXZtmJTr3H"
      },
      "outputs": [],
      "source": [
        "print_two_sample_unit_test_stats(shifts, gel_two_sample_tests, config_flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCLi_aJHTuuD"
      },
      "source": [
        "# A couple motivating examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxb7_a0PTw90"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NzMIq74Tx7F"
      },
      "outputs": [],
      "source": [
        "def make_mode_probs(probs, test_labels, num_classes=10):\n",
        "  mode_probs = np.empty((num_classes,))\n",
        "  for i in range(num_classes):\n",
        "    mode_probs[i] = np.sum(probs[test_labels == i])\n",
        "\n",
        "  return mode_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU8GZBpAT5Gl"
      },
      "source": [
        "## Evaluating Mode Dropping with one-sample exponential tilting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DULpP5pQT6Gq"
      },
      "source": [
        "Here, we recreate Figure 3(a) of the paper.\n",
        "\n",
        "The \"generative model\" here is 40k examples from the CIFAR10 training set, with up to 8 classes missing.\n",
        "\n",
        "For the mode dropping experiments, we remove examples from the last n classes. For example, if two modes are dropped, we remove labels 9 and 10. We use pool3 features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xaiTIEzT9KU"
      },
      "source": [
        "### Load features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxzT9U_lT_Oc"
      },
      "outputs": [],
      "source": [
        "data_dirn = 'cifar10_mode_drop_data/'\n",
        "cifar10_dropped_mode_data = dict()\n",
        "cifar10_mode_drop_gold_probs = dict()\n",
        "gel_one_sample_tests = dict()\n",
        "\n",
        "test_data = np.load(os.path.join(data_dirn, 'cifar10_test_pool3.npz'))\n",
        "test_feats = test_data['features']\n",
        "test_labels = test_data['labels']\n",
        "\n",
        "witness_data = np.load(os.path.join(\n",
        "    data_dirn, 'cifar10_train_valid_10k_pool3.npz'))\n",
        "witness_feats = witness_data['features'][:1024]\n",
        "\n",
        "all_train_data = np.load(\n",
        "    os.path.join(data_dirn, 'cifar10_train_valid_40k_pool3.npz'))\n",
        "\n",
        "for num_present_modes in range(2, 11, 2):\n",
        "  cifar10_dropped_mode_data[num_present_modes] = (\n",
        "      all_train_data['features'][all_train_data['labels'] \u003c num_present_modes])\n",
        "  cifar10_mode_drop_gold_probs[num_present_modes] = np.zeros((10,))\n",
        "  cifar10_mode_drop_gold_probs[num_present_modes][:num_present_modes] = (\n",
        "      1. / num_present_modes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX1DSzanUDkS"
      },
      "source": [
        "### Calculate kernel features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwUJmpsNUH86"
      },
      "outputs": [],
      "source": [
        "kernel_config_flags = get_kernel_config()\n",
        "kernel_features = dict()\n",
        "test_kernel_feats = kernel_matrix(\n",
        "    test_feats, witness_feats, kernel_config_flags)\n",
        "for num_present_modes in range(2, 11, 2):\n",
        "  kernel_features[num_present_modes] = kernel_matrix(\n",
        "      cifar10_dropped_mode_data[num_present_modes], witness_feats,\n",
        "      kernel_config_flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZRvLdjFULdA"
      },
      "source": [
        "### Calculate KGEL\n",
        "\n",
        "This code block recreates the results of Figure 3(a)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSL2mvjxUQ9T"
      },
      "outputs": [],
      "source": [
        "config_flags = get_gel_config()\n",
        "hellinger_distances = dict()\n",
        "for num_present_modes in range(2, 11, 2):\n",
        "  gel_one_sample_tests[num_present_modes] = OneSampleGEL(\n",
        "      config_flags, kernel_features[num_present_modes], test_kernel_feats)\n",
        "  outputs = gel_one_sample_tests[num_present_modes].calculate_gel()\n",
        "  per_sample_probs = outputs['probs']\n",
        "  mode_probs = make_mode_probs(per_sample_probs, test_labels)\n",
        "  num_missing_modes = 10 - num_present_modes\n",
        "  hellinger_distances[num_missing_modes] = hellinger_dist(\n",
        "      mode_probs, cifar10_mode_drop_gold_probs[num_present_modes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEsAznA8UUK1"
      },
      "source": [
        "### Calculate Hellinger Distances\n",
        "\n",
        "Hellinger distances calculated here are *slightly* different than what is reported in the paper, likely due to numerical precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JocNFyFaUVrQ"
      },
      "outputs": [],
      "source": [
        "for num_present_modes in range(2, 11, 2)[::-1]:\n",
        "  num_missing_modes = 10 - num_present_modes\n",
        "  print(\"%d missing modes, Hellinger distance: %.4f\"\n",
        "        % (num_missing_modes, hellinger_distances[num_missing_modes]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIsad0JHUYv7"
      },
      "source": [
        "In this example, we perform an experiment where the model distribution (the first half of the CIFAR10 training+validation sets) only has samples from class labels 0 and 1, while the test distribution (the second half of the CIFAR10 training+validation sets) only has samples from class labels 1 and 2. In the ideal scenario, the model and test probabilities for class 1 should sum to 1.0, while the other probabilities should sum to 0.0.\n",
        "\n",
        "The witness features are from the CIFAR10 test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUKeOQAPUaFO"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_htQIO2vUda-"
      },
      "outputs": [],
      "source": [
        "def make_two_modes(\n",
        "    model_data: dict, test_data: dict,\n",
        "    common_mode: int = 0, disjoint_mode1: int = 1, disjoint_mode2: int = 2,\n",
        "    num_feats_per_class = None):\n",
        "  \"\"\"Given two sets of features, construct two for use with two-sample tests.\n",
        "\n",
        "  The structure of the outputs have two sets of features. One has examples from\n",
        "  common_mode (class A) and disjoint_mode1 (class B). The second set has\n",
        "  examples from common_mode (class A) and disjoint_mode2 (class C).\"\"\"\n",
        "  assert disjoint_mode1 != disjoint_mode2\n",
        "  out1_data = dict()\n",
        "  model_feats = model_data['features']\n",
        "  model_labels = model_data['labels']\n",
        "  feats_common = model_feats[model_labels == common_mode][:num_feats_per_class]\n",
        "  assert feats_common.shape[0] == num_feats_per_class\n",
        "  feats_disjoint1 = model_feats[model_labels == disjoint_mode1][\n",
        "      :num_feats_per_class]\n",
        "  assert feats_disjoint1.shape[0] == num_feats_per_class\n",
        "  out1_data['features'] = np.concatenate(\n",
        "      [feats_common, feats_disjoint1], axis=0)\n",
        "  labels_list = [common_mode] * num_feats_per_class\n",
        "  labels_list += [disjoint_mode1] * num_feats_per_class\n",
        "  out1_data['labels'] = np.array(labels_list, dtype=np.int32)\n",
        "\n",
        "  out2_data = dict()\n",
        "  test_feats = test_data['features']\n",
        "  test_labels = test_data['labels']\n",
        "  feats_common2 = test_feats[test_labels == common_mode][:num_feats_per_class]\n",
        "  assert feats_common2.shape[0] == num_feats_per_class, feats_common2.shape[0]\n",
        "  feats_disjoint2 = test_feats[test_labels == disjoint_mode2][\n",
        "      :num_feats_per_class]\n",
        "  assert feats_disjoint2.shape[0] == num_feats_per_class\n",
        "\n",
        "  out2_data['features'] = np.concatenate(\n",
        "      [feats_common2, feats_disjoint2], axis=0)\n",
        "  labels_list = [common_mode] * num_feats_per_class\n",
        "  labels_list += [disjoint_mode2] * num_feats_per_class\n",
        "  out2_data['labels'] = np.array(labels_list, dtype=np.int32)\n",
        "\n",
        "  return out1_data, out2_data\n",
        "\n",
        "def make_label_balanced_model_and_test_data():\n",
        "  \"\"\"Make features with 2500 egs per class for data with 5000 egs per class.\"\"\"\n",
        "  data_dirn = 'cifar10_mode_drop_data/'\n",
        "  all_data = np.load(\n",
        "      os.path.join(data_dirn, 'cifar10_train_valid_50k_pool3.npz'))\n",
        "  all_data_feats = all_data['features']\n",
        "  all_data_labels = all_data['labels']\n",
        "  out_feats_model = list()\n",
        "  out_labels_model = list()\n",
        "  out_feats_test = list()\n",
        "  out_labels_test = list()\n",
        "  num_examples_per_sample_set = 2500\n",
        "  for label in range(10):\n",
        "    per_class_feats = all_data_feats[all_data_labels == label]\n",
        "    per_class_labels = all_data_labels[all_data_labels == label]\n",
        "    out_feats_model.append(per_class_feats[:num_examples_per_sample_set])\n",
        "    out_labels_model.append(per_class_labels[:num_examples_per_sample_set])\n",
        "    out_feats_test.append(per_class_feats[num_examples_per_sample_set:])\n",
        "    out_labels_test.append(per_class_labels[num_examples_per_sample_set:])\n",
        "\n",
        "  out_model_data = dict(\n",
        "      features=np.concatenate(out_feats_model, axis=0),\n",
        "      labels=np.concatenate(out_labels_model))\n",
        "  out_test_data = dict(\n",
        "      features=np.concatenate(out_feats_test, axis=0),\n",
        "      labels=np.concatenate(out_labels_test))\n",
        "\n",
        "  return out_model_data, out_test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhlQaJr2Ug2k"
      },
      "source": [
        "### Load features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2jcJE-YUjU-"
      },
      "source": [
        "Make two sets of features\n",
        "\n",
        "1. For the \"model\", keep features from classes 0 and 1\n",
        "2. For the \"test\", keep features from classes 1 and 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EFrN9j2Ulqn"
      },
      "outputs": [],
      "source": [
        "model_data, test_data = make_label_balanced_model_and_test_data()\n",
        "num_feats_per_class = 2500  # using 50k set, 2500 egs/class\n",
        "model_two_classes, test_two_classes = make_two_modes(\n",
        "    model_data, test_data, num_feats_per_class=num_feats_per_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXLoj7xFUnig"
      },
      "source": [
        "### Calculate kernel features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kssIgrXUrai"
      },
      "outputs": [],
      "source": [
        "kernel_config_flags = get_kernel_config()\n",
        "witness_data = np.load(os.path.join(\n",
        "    data_dirn, 'cifar10_test_pool3.npz'))\n",
        "witness_feats = witness_data['features'][:1024]\n",
        "model_kgel_data = dict(labels=model_two_classes['labels'])\n",
        "test_kgel_data = dict(labels=test_two_classes['labels'])\n",
        "model_kgel_data['features'] = kernel_matrix(\n",
        "    model_two_classes['features'], witness_feats, kernel_config_flags)\n",
        "test_kgel_data['features'] = kernel_matrix(\n",
        "    test_two_classes['features'], witness_feats, kernel_config_flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liLC7t4KUuQR"
      },
      "source": [
        "### Calculate KGEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7bJxFHBUxhr"
      },
      "outputs": [],
      "source": [
        "config_flags = get_gel_config()\n",
        "gel_two_sample_test = TwoSampleGEL(\n",
        "    config_flags, model_kgel_data['features'], test_kgel_data['features'])\n",
        "out_dict = gel_two_sample_test.calculate_gel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBuZQ5N6UyfN"
      },
      "source": [
        "### Extract Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWM8bX_VU0nK"
      },
      "outputs": [],
      "source": [
        "model_probs = np.array(\n",
        "    [out_dict['model_probs'][:num_feats_per_class].sum(),\n",
        "     out_dict['model_probs'][num_feats_per_class:].sum()])\n",
        "\n",
        "test_probs = np.array(\n",
        "    [out_dict['test_probs'][:num_feats_per_class].sum(),\n",
        "     out_dict['test_probs'][num_feats_per_class:].sum()])\n",
        "\n",
        "print('Ideal probability of the common mode is 1.0')\n",
        "print('Ideal probability of the disjoint mode is 0.0')\n",
        "\n",
        "print('Model probability of the common mode is', model_probs[0])\n",
        "print('Model probability of the disjoint mode is', model_probs[1])\n",
        "\n",
        "print('Test probability of the common mode is', test_probs[0])\n",
        "print('Test probability of the disjoint mode is', test_probs[1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1q21iNK4Uhkdehj3FrCfmeW2lzPGksQJm",
          "timestamp": 1687020660673
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
